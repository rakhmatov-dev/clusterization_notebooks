{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42ab125-5a92-43c5-a980-4083fbe19302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import unicodedata as ud\n",
    "import pymorphy2\n",
    "from gensim import models\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models.phrases import Phrases\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8327a8e2-5e2c-4637-8339-31e8d688c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "df_raw = pd.read_excel('elastic_lang.xlsx')\n",
    "df_raw['col'] = df_raw['cyrillic'].str.lower()\n",
    "df_lower = df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2335d6c8-a20b-4af8-9f16-2cd09725a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PreProcess data (drop unused columns, reindex)\n",
    "nan_value = float(\"NaN\")\n",
    "df_lower.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "df_lower.dropna(subset = [\"cyrillic\"], inplace=True)\n",
    "df_lower = df_lower.drop(columns=['Unnamed: 0'])\n",
    "df_lower = df_lower.reset_index(drop = True)\n",
    "df_lower['cyrillic'] = df_lower['cyrillic'].str.lower()\n",
    "texts = df_lower['cyrillic'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "680d4a18-1d79-4c3f-969a-be437f3a632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 996929/996929 [02:52<00:00, 5782.24it/s]\n"
     ]
    }
   ],
   "source": [
    "## Normalizing tokens\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\\\n",
    "\n",
    "df_lower['cyrillic'] = df_lower['cyrillic'].str.lstrip()\n",
    "corpus = df_lower['cyrillic'].str.cat(sep = ' ')\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    l = len(tokens)\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return [morph.parse(tokens[i])[0].normal_form for i in tqdm(range(l))]\n",
    "\n",
    "\n",
    "data_raw_next = []\n",
    "\n",
    "data_raw_next = normalize_tokens(corpus.split(sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327ea7c-7755-421e-af43-527b44ff028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"df_lower_cyr_normalized.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(data_raw_next, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c84733fd-4039-4ec0-ac30-89e218830dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add to df_lower normalized names\n",
    "\n",
    "df_lower['num_of_cyr'] = ''\n",
    "df_lower['tokenized_cyr'] = ''\n",
    "df_lower['num_of_cyr'] = [len(doc.split(sep = ' ')) for doc in df_lower['cyrillic']]\n",
    "k=0\n",
    "tmp_list = []\n",
    "for num in df_lower['num_of_cyr']:\n",
    "    tmp_list.append(' '.join(data_raw_next[k:(k + int(num))]))\n",
    "    k+=num\n",
    "df_lower['tokenized_cyr'] = tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cac96f-9c2b-441d-9428-fb6d13907f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threads ok\n"
     ]
    }
   ],
   "source": [
    "## Vectorizing normalized names\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "        min_count=12,\n",
    "        vector_size=10,\n",
    "        window=2,\n",
    "        negative=10,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        sample=6e-5,\n",
    "        sg=1)\n",
    "\n",
    "data = [doc.split() for doc in df_lower['tokenized_cyr']]\n",
    "    \n",
    "print('threads ok')\n",
    "w2v_model.build_vocab(data)\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "653f1981-0216-43a8-b77a-1605398ddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make average vector from number of word vectors\n",
    "\n",
    "def normalize_token(token):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return morph.parse(token.lower())[0].normal_form \n",
    "\n",
    "\n",
    "def mean_for_vecs(sentence):\n",
    "    if len(sentence.split(sep = ' ')) > 1:\n",
    "        text_vector = np.mean([w2v_model.wv[word] for word in sentence.split(sep = ' ')], axis=0)\n",
    "    else:\n",
    "        text_vector = w2v_model.wv[word]\n",
    "    return text_vector\n",
    "\n",
    "df_mean_vec = pd.DataFrame(columns = ['sentence', 'vec'])\n",
    "sentences =[]\n",
    "df_lower = df_lower.reset_index(drop=True)\n",
    "vecs = []\n",
    "for i in range(len(df_lower['tokenized_cyr'])):\n",
    "#     if df_lower.col[i]:\n",
    "    try:\n",
    "        vecs.append(mean_for_vecs(df_lower['tokenized_cyr'][i]))\n",
    "        sentences.append(df_lower['tokenized_cyr'][i])\n",
    "    except Exception as e :\n",
    "        pass\n",
    "\n",
    "df_mean_vec['sentence'] = sentences\n",
    "df_mean_vec['vec'] = vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9bba528-c1ae-4f80-afdc-16343fffb9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27409828,  0.48373708,  0.4384637 , ..., -0.36254954,\n",
       "        -0.16276667,  0.4101392 ],\n",
       "       [-0.12132712,  0.10477108,  0.23433994, ..., -0.13063593,\n",
       "        -0.2860515 ,  0.07591018],\n",
       "       [-0.12100955,  0.00312524,  0.19290012, ..., -0.22562097,\n",
       "        -0.43487152, -0.36587086],\n",
       "       ...,\n",
       "       [-0.26895905,  0.39840204,  0.3154568 , ...,  0.03128564,\n",
       "        -0.21732116, -0.14214496],\n",
       "       [ 0.0863111 ,  0.0695575 ,  0.34955055, ...,  0.20414725,\n",
       "        -0.367661  , -0.07540943],\n",
       "       [ 0.07225259,  0.35874903,  0.37715837, ..., -0.18512839,\n",
       "        -0.2612232 , -0.16917929]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(df_mean_vec['vec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ad1ebf8-8c7f-4073-b761-0411208b04c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assigned_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Temp/ipykernel_11060/3468692560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mcluster_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mcluster_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massigned_clusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mdf_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cluster'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0massigned_clusters\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'assigned_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "## Model of Clustering\n",
    "\n",
    "\n",
    "X = np.array(list(df_mean_vec['vec']))\n",
    "\n",
    "# from nltk.cluster import KMeansClusterer\n",
    "# import nltk\n",
    "# NUM_CLUSTERS=200\n",
    "# kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters = True)\n",
    "# assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=150, init='k-means++', n_init=10, max_iter=50, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='auto').fit_predict(X)\n",
    "\n",
    "\n",
    "words = df_mean_vec['sentence']\n",
    "cluster_dict = {}\n",
    "for i, word in enumerate(words):  \n",
    "    cluster_dict[str(word)] = str(assigned_clusters[i])\n",
    "\n",
    "df_clusters = pd.DataFrame({'word': words, 'cluster': assigned_clusters})\n",
    "df_clusters.to_excel('tmp_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e50a744-6a37-419b-835f-685820e3bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for i, word in enumerate(words):  \n",
    "    cluster_dict[str(word)] = str(kmeans[i])\n",
    "\n",
    "df_clusters = pd.DataFrame({'word': words, 'cluster': kmeans})\n",
    "df_clusters.to_excel('tmp_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c18de-388e-410a-afb0-c42653730e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for i, word in enumerate(words):  \n",
    "    cluster_dict[str(word)] = str(assigned_clusters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191786e-9b0b-4b49-935e-0daf08398a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame({'word': words, 'cluster': assigned_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42084320-c064-44a2-8e48-0f5a57b93984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters.to_excel('tmp_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee962b26-4d83-4b20-91db-c10f9de94ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=100)\n",
    "kmeans.fit(X)\n",
    " \n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "print (\"Centroids data\")\n",
    "print (centroids)\n",
    " \n",
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
